{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44fb21d-ddc3-4179-b0d4-e9adcdaaa900",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab345dc3-b810-4b7d-85b3-2d3c2f33ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge opencv -y\n",
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1745f-5403-470d-aa82-23a7e6206c58",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dbe1d-c37d-4588-8d5e-5d76f7363bc4",
   "metadata": {},
   "source": [
    "# DeepfakeTIMIT: Deepfakes\n",
    "Any publication (eg. conference paper, journal article, technical report, book chapter, etc) resulting from the usage of DeepfakeTIMIT must cite the following paper:\n",
    "\n",
    "    P. Korshunov and S. Marcel,\n",
    "    DeepFakes: a New Threat to Face Recognition? Assessment and Detection.\n",
    "    arXiv https://arxiv.org/abs/1812.08685 and Idiap Research Report (http://publications.idiap.ch/index.php/publications/show/3988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd0cb406-8775-44a2-b065-abfe8cc3a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-05 23:27:26--  https://my.pcloud.com/publink/show?code=XZLGvd7ZI9LjgIy7iOLzXBG5RNJzGFQzhTRy\n",
      "Resolving my.pcloud.com (my.pcloud.com)... 45.131.244.10, 45.131.247.13, 45.131.247.15, ...\n",
      "Connecting to my.pcloud.com (my.pcloud.com)|45.131.244.10|:443... connected.\n",
      "WARNING: cannot verify my.pcloud.com's certificate, issued by ‘CN=SwissSign RSA TLS OV ICA 2022 - 1,O=SwissSign AG,C=CH’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://u.pcloud.link/publink/show?code=XZLGvd7ZI9LjgIy7iOLzXBG5RNJzGFQzhTRy [following]\n",
      "--2025-02-05 23:27:26--  https://u.pcloud.link/publink/show?code=XZLGvd7ZI9LjgIy7iOLzXBG5RNJzGFQzhTRy\n",
      "Resolving u.pcloud.link (u.pcloud.link)... 74.120.8.115, 74.120.9.94, 74.120.8.110, ...\n",
      "Connecting to u.pcloud.link (u.pcloud.link)|74.120.8.115|:443... connected.\n",
      "WARNING: cannot verify u.pcloud.link's certificate, issued by ‘CN=GoGetSSL RSA DV CA,O=GoGetSSL,L=Riga,C=LV’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-02-05 23:27:27 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate -O DeepfakeTIMIT.tar.gz \"https://zenodo.org/records/4068245/files/DeepfakeTIMIT.tar.gz?download=1\"\n",
    "!tar -xf DeepfakeTIMIT.tar.gz\n",
    "!rm DeepfakeTIMIT.tar.gz\n",
    "!wget --no-check-certificate \"https://my.pcloud.com/publink/show?code=XZLGvd7ZI9LjgIy7iOLzXBG5RNJzGFQzhTRy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b7557-5a6c-4ea2-9cd1-e4a0b2583038",
   "metadata": {},
   "source": [
    "# VidTIMIT Audio-Video Dataset: True Images\n",
    "The VidTIMIT dataset is Copyright © 2001 Conrad Sanderson.\n",
    "\n",
    "Distribution and research usage of this dataset is permitted under the following conditions:\n",
    "\n",
    "    1. This notice is left intact and not modified in any way.\n",
    "    2. The dataset is provided as is. There is no warranty as to the fitness for any particular purpose.\n",
    "    3. The author of the dataset is not responsible for any direct or indirect losses resulting from the use of the dataset.\n",
    "    4. Any publication (eg. conference paper, journal article, technical report, book chapter, etc) resulting from the usage of VidTIMIT must cite the following paper:\n",
    "        C. Sanderson and B.C. Lovell\n",
    "        Multi-Region Probabilistic Histograms for Robust and Scalable Identity Inference.\n",
    "        Lecture Notes in Computer Science (LNCS), Vol. 5558, pp. 199-208, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de6ece6-5d83-4fa3-9ce6-542fceb2c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/record/158963/files/mjar0.zip...\n",
      "Downloaded: VidTIMIT/mjar0.zip\n"
     ]
    }
   ],
   "source": [
    "# List of file names\n",
    "file_names = [\n",
    "    \"fadg0\", \"faks0\", \"fcft0\", \"fcmh0\", \"fdac1\", \"fdrd1\", \"fedw0\", \"felc0\", \"fjas0\",    \"fjem0\", \"fjre0\", \"fjwb0\", \"fkms0\", \"fram1\",\n",
    "    \"mccs0\", \"mcem0\", \"mdab0\", \"mdbb0\", \"mdld0\", \"mgwt0\", \"mjar0\", \"mjsw0\", \"mmdb1\", \"mmdm2\", \"mpdf0\", \"mpgl0\", \"mrcz0\", \"mrgg0\", \"mrjo0\",\n",
    "    \"msjs1\", \"mstk0\", \"mwbt0\"\n",
    "]\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://zenodo.org/record/158963/files/{}.zip\"\n",
    "\n",
    "# Download folder\n",
    "download_folder = \"VidTIMIT\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Function to download files\n",
    "def download_file(file_name):\n",
    "    url = base_url.format(file_name)\n",
    "    local_filename = os.path.join(download_folder, f\"{file_name}.zip\")\n",
    "    \n",
    "    print(f\"Downloading {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded: {local_filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}, Status Code: {response.status_code}\")\n",
    "\n",
    "# Download each file one at a time\n",
    "for file_name in file_names:\n",
    "    download_file(file_name)\n",
    "\n",
    "# Function to extract and delete zip files\n",
    "def extract_and_delete_zip(zip_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_folder)\n",
    "        print(f\"Extracted: {zip_path}\")\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Deleted: {zip_path}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_path} is not a valid zip file.\")\n",
    "\n",
    "# Process all zip files in the folder\n",
    "for file in os.listdir(download_folder):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(download_folder, file)\n",
    "        extract_and_delete_zip(zip_path)\n",
    "\n",
    "print(\"All zip files extracted and deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3e83a-8601-4ef7-869c-c6d04b73767e",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbacc42-6ae7-4ff2-a488-c391f7fccd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9923c071-7084-4883-a0da-cd2c64b9bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd1b09-03e6-499b-9749-7a9a4be85da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b4336-7655-4f52-9026-a643fee59053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_video(video_path, mtcnn, face_encoder, device=DEVICE, frame_interval=5):\n",
    "    \"\"\"Extracts and encodes faces from a video efficiently.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    face_embeddings = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        faces, _ = mtcnn.detect(frame_rgb)\n",
    "        if faces is not None:\n",
    "            for box in faces:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face = frame_rgb[y1:y2, x1:x2]\n",
    "                face_tensor = transform(face).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    embedding = face_encoder(face_tensor)\n",
    "                face_embeddings.append(embedding.cpu().numpy())\n",
    "    cap.release()\n",
    "    return np.mean(face_embeddings, axis=0) if face_embeddings else None\n",
    "\n",
    "def detect_deepfake(real_video, unknown_video, model, threshold=0.7):\n",
    "    \"\"\"Compares two videos and determines if the unknown video is a deepfake.\"\"\"\n",
    "    real_embedding = extract_faces_from_video(real_video, mtcnn, face_encoder)\n",
    "    unknown_embedding = extract_faces_from_video(unknown_video, mtcnn, face_encoder)\n",
    "\n",
    "    if real_embedding is None or unknown_embedding is None:\n",
    "        return \"Error: Could not extract faces from one or both videos\"\n",
    "\n",
    "    real_tensor = torch.tensor(real_embedding).float().unsqueeze(0).to(DEVICE)\n",
    "    unknown_tensor = torch.tensor(unknown_embedding).float().unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        similarity = model(real_tensor, unknown_tensor).item()\n",
    "\n",
    "    return \"REAL\" if similarity > threshold else \"DEEPFAKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d99d91-59d1-447d-a741-c251cc85d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "real_video_path = \"real.mp4\"\n",
    "unknown_video_path = \"unknown.mp4\"\n",
    "\n",
    "result = detect_deepfake(real_video_path, unknown_video_path, model)\n",
    "print(f\"Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab948468-2a6c-4ec6-91b6-6b70998de2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Face Detector (MTCNN) and Feature Extractor (FaceNet)\n",
    "mtcnn = MTCNN(keep_all=True, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "face_encoder = InceptionResnetV1(pretrained='vggface2').eval().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define Siamese Network with Temporal Modeling of Facial Movements\n",
    "class SiameseNetworkTemporal(nn.Module):\n",
    "    def __init__(self, hidden_dim=16, num_layers=2):\n",
    "        super(SiameseNetworkTemporal, self).__init__()\n",
    "        self.resnet = models.resnet50(models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Remove final classification layer\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1_seq, x2_seq):\n",
    "        batch_size, seq_len, c, h, w = x1_seq.shape  # Extract dimensions\n",
    "        x1_seq = x1_seq.view(batch_size * seq_len, c, h, w)  # Flatten batch & sequence\n",
    "        x2_seq = x2_seq.view(batch_size * seq_len, c, h, w)\n",
    "        \n",
    "        f1_seq = self.resnet(x1_seq).view(batch_size, seq_len, -1)  # Extract features\n",
    "        f2_seq = self.resnet(x2_seq).view(batch_size, seq_len, -1)\n",
    "\n",
    "        f1_seq, _ = self.lstm(f1_seq)  # Pass through LSTM for facial movement analysis\n",
    "        f2_seq, _ = self.lstm(f2_seq)\n",
    "        \n",
    "        f1 = torch.mean(f1_seq, dim=1)  # Average hidden states for temporal aggregation\n",
    "        f2 = torch.mean(f2_seq, dim=1)\n",
    "        \n",
    "        f1, f2 = self.fc(f1), self.fc(f2)  # Fully connected layers\n",
    "        similarity = torch.cosine_similarity(f1, f2)\n",
    "        return similarity\n",
    "\n",
    "# Initialize Model with Temporal Movement Analysis\n",
    "model = SiameseNetworkTemporal().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96ef5c-6d7a-46ad-98e3-cce1a46c009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy input tensors to simulate a batch of video frames (batch_size=2, seq_len=5, channels=3, height=224, width=224)\n",
    "dummy_x1 = torch.randn(2, 5, 3, 224, 224).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "dummy_x2 = torch.randn(2, 5, 3, 224, 224).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Check model output\n",
    "try:\n",
    "    output = model(dummy_x1, dummy_x2)\n",
    "    print(\"Model Output:\", output)\n",
    "    print(\"Output Shape:\", output.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183ae7b7-ac8d-4d7e-b735-c513e6e38e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa2dd99-77c4-4ef3-bec8-d5c9ad9ddde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, c, h, w = dummy_x1.shape  # Extract dimensions\n",
    "x1_seq = dummy_x1.view(batch_size * seq_len, c, h, w)  # Flatten batch & sequence\n",
    "x1 = model.resnet(x1_seq).view(batch_size, seq_len, -1)\n",
    "# model.lstm(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb7c2a6-4109-4558-9273-fc34ef1ba4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0267,  0.1654,  0.0804, -0.0120, -0.1208,  0.0290, -0.1028,\n",
       "            0.0491, -0.0563,  0.0492,  0.0735, -0.0279,  0.0368, -0.0032,\n",
       "            0.0289,  0.0395, -0.0313,  0.1550, -0.1979,  0.0280, -0.1185,\n",
       "            0.0214,  0.1448,  0.0384, -0.1222, -0.2314, -0.0457, -0.0318,\n",
       "            0.1365,  0.0237, -0.0080,  0.0316],\n",
       "          [-0.0493,  0.2520,  0.1263,  0.0124, -0.1799,  0.0362, -0.1554,\n",
       "            0.0059, -0.0961,  0.0586,  0.0947, -0.0388,  0.1078,  0.0303,\n",
       "            0.0412,  0.0412, -0.0227,  0.1311, -0.1563,  0.0477, -0.1851,\n",
       "            0.0142,  0.1342,  0.0705, -0.1141, -0.1966, -0.0291, -0.0572,\n",
       "            0.1406,  0.0244,  0.0275,  0.0513],\n",
       "          [-0.0152,  0.2395,  0.0760,  0.0304, -0.1726,  0.0456, -0.1681,\n",
       "           -0.0222, -0.1296,  0.0527,  0.1342, -0.0563,  0.0582, -0.0643,\n",
       "            0.0433,  0.0600, -0.0213,  0.1027, -0.1112,  0.0169, -0.2132,\n",
       "           -0.0124,  0.1067,  0.0448, -0.1113, -0.1838, -0.0246, -0.0724,\n",
       "            0.1162, -0.0098,  0.0541,  0.0728],\n",
       "          [ 0.0342,  0.2110,  0.0419,  0.0312, -0.1665,  0.0835, -0.1113,\n",
       "           -0.0544, -0.1666,  0.0204,  0.1340, -0.0515,  0.0797, -0.0318,\n",
       "            0.0335,  0.0215,  0.0208,  0.0639, -0.0799,  0.0608, -0.1916,\n",
       "           -0.0153,  0.1053,  0.0118, -0.1046, -0.1474, -0.0547, -0.0398,\n",
       "            0.1295, -0.0265,  0.0507,  0.0510],\n",
       "          [ 0.1039,  0.2091,  0.0487, -0.0329, -0.1174,  0.1304, -0.1264,\n",
       "           -0.0375, -0.1415,  0.0083,  0.1446, -0.0905,  0.1036,  0.0992,\n",
       "            0.0760,  0.0153,  0.0450,  0.0313, -0.0183,  0.0869, -0.1439,\n",
       "           -0.0468,  0.1019, -0.0274, -0.1046, -0.0792, -0.0444, -0.0236,\n",
       "            0.1132, -0.0695,  0.0549,  0.0654]],\n",
       " \n",
       "         [[ 0.0422,  0.0239,  0.0774, -0.0485, -0.1066,  0.0591, -0.0959,\n",
       "           -0.0358, -0.1103,  0.0204,  0.0715, -0.0017,  0.0195,  0.0192,\n",
       "            0.0404,  0.0758, -0.0543,  0.0622, -0.1202,  0.0370, -0.0264,\n",
       "           -0.0701,  0.1013,  0.0698, -0.1861, -0.2038,  0.0241, -0.0721,\n",
       "            0.1600,  0.1013, -0.0322,  0.1305],\n",
       "          [ 0.0228,  0.1133,  0.1194, -0.0498, -0.1682,  0.0625, -0.1239,\n",
       "            0.0102, -0.1193,  0.1083,  0.0956,  0.0153,  0.0362,  0.0226,\n",
       "            0.0321,  0.1429, -0.0211,  0.1002, -0.1562,  0.0113,  0.0181,\n",
       "           -0.0239,  0.1177,  0.0570, -0.1245, -0.1850,  0.0246, -0.0478,\n",
       "            0.0800,  0.0574, -0.0140,  0.1227],\n",
       "          [-0.0249,  0.1726,  0.1719, -0.0485, -0.1819,  0.0844, -0.1544,\n",
       "           -0.0264, -0.1021,  0.1130,  0.1065,  0.0445,  0.0788,  0.0720,\n",
       "           -0.0051,  0.1440, -0.0260,  0.0843, -0.1480,  0.0070, -0.0189,\n",
       "            0.0038,  0.1435,  0.0290, -0.1202, -0.2124,  0.0003, -0.0464,\n",
       "            0.0959,  0.0830, -0.0595,  0.0897],\n",
       "          [-0.0174,  0.1478,  0.1890, -0.0048, -0.1739,  0.1038, -0.1553,\n",
       "           -0.0793, -0.1262,  0.1137,  0.1196,  0.0211,  0.1152,  0.1413,\n",
       "            0.0116,  0.1026, -0.0301,  0.0697, -0.1325,  0.0050, -0.0123,\n",
       "            0.0125,  0.1429,  0.0285, -0.0876, -0.1729, -0.0085, -0.0744,\n",
       "            0.0879,  0.0514, -0.0457,  0.0570],\n",
       "          [-0.0198,  0.0343,  0.2029,  0.0791, -0.1896,  0.0958, -0.1670,\n",
       "           -0.0670, -0.1201,  0.1284,  0.1022,  0.0209,  0.0995,  0.0999,\n",
       "            0.0052,  0.0963, -0.0324,  0.0557, -0.1027, -0.0172,  0.0268,\n",
       "            0.0185,  0.0898,  0.0846, -0.0720, -0.1295,  0.0331, -0.0708,\n",
       "            0.0595,  0.0860, -0.0591,  0.1079]]], grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[ 0.1598, -0.2109,  0.1920, -0.4778, -0.5006, -0.3306, -0.0239,\n",
       "            -0.0071,  0.0447, -0.2683, -0.0307, -0.2546,  0.0281, -0.2309,\n",
       "            -0.2664, -0.0846],\n",
       "           [ 0.0061, -0.0980,  0.1481,  0.4092,  0.0210,  0.2823, -0.0590,\n",
       "            -0.5463,  0.1291, -0.2434, -0.3103, -0.0770,  0.0735, -0.1255,\n",
       "            -0.0318,  0.0384]],\n",
       "  \n",
       "          [[ 0.2994, -0.3822, -0.2518, -0.0379,  0.0537, -0.1648,  0.0696,\n",
       "            -0.1246, -0.0621, -0.0216, -0.3361,  0.3844, -0.2120, -0.1491,\n",
       "            -0.0137, -0.0760],\n",
       "           [ 0.4077, -0.5468, -0.2649,  0.0399,  0.3969, -0.3272, -0.2499,\n",
       "             0.2515, -0.0014,  0.1350, -0.0431, -0.3666, -0.0583,  0.1905,\n",
       "             0.4713, -0.0938]],\n",
       "  \n",
       "          [[ 0.1039,  0.2091,  0.0487, -0.0329, -0.1174,  0.1304, -0.1264,\n",
       "            -0.0375, -0.1415,  0.0083,  0.1446, -0.0905,  0.1036,  0.0992,\n",
       "             0.0760,  0.0153],\n",
       "           [-0.0198,  0.0343,  0.2029,  0.0791, -0.1896,  0.0958, -0.1670,\n",
       "            -0.0670, -0.1201,  0.1284,  0.1022,  0.0209,  0.0995,  0.0999,\n",
       "             0.0052,  0.0963]],\n",
       "  \n",
       "          [[-0.0313,  0.1550, -0.1979,  0.0280, -0.1185,  0.0214,  0.1448,\n",
       "             0.0384, -0.1222, -0.2314, -0.0457, -0.0318,  0.1365,  0.0237,\n",
       "            -0.0080,  0.0316],\n",
       "           [-0.0543,  0.0622, -0.1202,  0.0370, -0.0264, -0.0701,  0.1013,\n",
       "             0.0698, -0.1861, -0.2038,  0.0241, -0.0721,  0.1600,  0.1013,\n",
       "            -0.0322,  0.1305]]], grad_fn=<StackBackward0>),\n",
       "  tensor([[[ 0.2737, -0.4475,  0.4165, -0.5856, -0.6338, -0.4554, -0.1450,\n",
       "            -0.0118,  0.1711, -0.4430, -0.2194, -0.4328,  0.1487, -0.2786,\n",
       "            -0.9165, -0.1944],\n",
       "           [ 0.0238, -0.3004,  0.2822,  0.7694,  0.0286,  0.5016, -0.6004,\n",
       "            -0.6844,  0.2424, -0.4129, -1.1045, -0.2503,  0.4244, -0.2196,\n",
       "            -0.1623,  0.2708]],\n",
       "  \n",
       "          [[ 0.5223, -0.7936, -0.5440, -0.1045,  0.1020, -0.4241,  0.1436,\n",
       "            -0.1815, -0.2807, -0.2496, -0.4643,  0.9022, -0.5272, -0.2414,\n",
       "            -0.0238, -0.2403],\n",
       "           [ 0.5173, -1.0871, -1.1257,  0.1323,  0.5934, -0.5454, -0.3135,\n",
       "             0.3025, -0.0069,  0.7822, -0.0607, -0.5164, -0.2302,  0.3517,\n",
       "             0.7012, -0.1501]],\n",
       "  \n",
       "          [[ 0.1714,  0.3957,  0.0930, -0.0707, -0.2288,  0.3324, -0.2378,\n",
       "            -0.0778, -0.3212,  0.0155,  0.2903, -0.2151,  0.2059,  0.1875,\n",
       "             0.1548,  0.0339],\n",
       "           [-0.0453,  0.0568,  0.3592,  0.1398, -0.3503,  0.2226, -0.3152,\n",
       "            -0.1241, -0.2117,  0.2471,  0.1846,  0.0391,  0.2074,  0.2072,\n",
       "             0.0122,  0.1852]],\n",
       "  \n",
       "          [[-0.0639,  0.3207, -0.3841,  0.0667, -0.2324,  0.0396,  0.3116,\n",
       "             0.0792, -0.2157, -0.4805, -0.0976, -0.0597,  0.3041,  0.0457,\n",
       "            -0.0155,  0.0717],\n",
       "           [-0.1137,  0.1341, -0.2686,  0.0750, -0.0558, -0.1406,  0.2460,\n",
       "             0.1339, -0.3287, -0.4259,  0.0495, -0.1509,  0.2721,  0.1898,\n",
       "            -0.0535,  0.2728]]], grad_fn=<StackBackward0>)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d862bd4-7c98-4375-8c49-8c8284c98194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba80ee8-192e-4532-959b-ba00cde2ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
